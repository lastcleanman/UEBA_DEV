import pandas as pd
from sqlalchemy import create_engine, text
import json, glob, os
import xml.etree.ElementTree as ET
from core.utils import get_logger

logger = get_logger("Plugin-Input")

def fetch_db(source, last_updated):
    db_type = source.get("type", "").lower()
    db_name = source.get("database")
    table = source.get("table_name")
    
    # â­ï¸ ìˆ˜ì •: DB í…Œì´ë¸”ë„ ê¸°ë³¸ ì›Œí„°ë§ˆí¬ ì»¬ëŸ¼ì„ ì§€ì •í•˜ì—¬ ëˆ„ë½ ë°©ì§€
    w_col = source.get("watermark_col", "timestamp") 
    
    url = f"postgresql+psycopg2://{source['user']}:{source['password']}@{source['host']}:{source['port']}/{db_name}" if "postgres" in db_type else f"mysql+pymysql://{source['user']}:{source['password']}@{source['host']}:{source['port']}/{db_name}"
    
    engine = create_engine(url, pool_pre_ping=True)
    query = f"SELECT * FROM {table} WHERE {w_col} > :last_updated" if w_col else f"SELECT * FROM {table}"
    
    with engine.connect() as conn:
        df = pd.read_sql(text(query), conn, params={"last_updated": last_updated}) if ":last_updated" in query else pd.read_sql(text(query), conn)
    logger.info(f"âœ… [{source['name']}] ì¶”ì¶œ ì™„ë£Œ ({len(df)}ê±´) / í…Œì´ë¸”: {table}")
    return df

def get_hr_lookup(global_config):
    conf = next((s for s in global_config.get("sources", []) if s.get("name") == "ueba_mariaDB"), None)
    if not conf: return None
    try:
        hr_df = fetch_db(conf, "1970-01-01 00:00:00")
        if hr_df is not None and not hr_df.empty:
            id_col = 'employee_id' if 'employee_id' in hr_df.columns else ('emp_id' if 'emp_id' in hr_df.columns else hr_df.columns[0])
            name_col = 'name_kr' if 'name_kr' in hr_df.columns else ('emp_name' if 'emp_name' in hr_df.columns else hr_df.columns[1])
            return dict(zip(hr_df[id_col].astype(str), hr_df[name_col].astype(str)))
    except Exception as e: logger.warning(f"âš ï¸ HR ë¡œë“œ ì‹¤íŒ¨: {e}")
    return None

def get_parser_info(source_name, global_config):
    base_dir = global_config.get("system", {}).get("base_dir", "/UEBA_DEV")
    xml_path = os.path.join(base_dir, "conf", "parsers", f"{source_name}.xml")
    
    fmt, sep, columns = "json", ",", None
    if os.path.exists(xml_path):
        try:
            tree = ET.parse(xml_path)
            root = tree.getroot()
            fmt = root.get("format", "json")
            sep = root.get("delimiter", ",")
            if fmt == "delimited":
                # â­ï¸ í•µì‹¬ ìˆ˜ì •: 'field' -> './/field' ë¡œ ë³€ê²½í•˜ì—¬ ë‚´ë¶€ì— ìˆ¨ì€ íƒœê·¸ê¹Œì§€ ëª¨ë‘ ì°¾ìŠµë‹ˆë‹¤.
                fields = sorted(root.findall('.//field'), key=lambda x: int(x.get('index', 0)))
                columns = [f.get('target') for f in fields]
        except Exception as e:
            logger.error(f"âŒ XML íŒŒì‹± ì—ëŸ¬({source_name}): {e}")
    return fmt, sep, columns

def fetch_data(source, global_config, last_updated="1970-01-01 00:00:00"):
    source_name = source.get("name")
    try:
        df = None
        if source.get("type") in ["postgres", "mariadb"]: 
            df = fetch_db(source, last_updated)
            
        elif source.get("type") == "file":
            files = glob.glob(source.get("path", ""))
            
            # â­ï¸ ë™ì  íŒŒì„œ ì •ë³´ ë¡œë“œ
            fmt, sep, columns = get_parser_info(source_name, global_config)
            
            df_list = []
            for f in files:
                if fmt == "delimited" and columns:
                    # ì •ê·œì‹ ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•œ íŠ¹ìˆ˜ë¬¸ì ì´ìŠ¤ì¼€ì´í”„ (| ë“±)
                    safe_sep = f"\\{sep}" if sep in "|^*." else sep
                    df_list.append(pd.read_csv(f, sep=safe_sep, header=None, names=columns, engine='python', dtype=str))
                elif fmt == "json":
                    df_list.append(pd.read_json(f, lines=True))
                else:
                    df_list.append(pd.read_csv(f))
                    
            if df_list: 
                df = pd.concat(df_list, ignore_index=True)
                
                # ì›Œí„°ë§ˆí¬ (ì‹œê°„) ì²˜ë¦¬
                w_col = source.get("watermark_col", "timestamp")
                if w_col not in df.columns and "@timestamp" in df.columns: 
                    w_col = "@timestamp"
                    
                if w_col in df.columns:
                    original_count = len(df)
                    
                    # â­ï¸ 1. ë³´ì´ì§€ ì•ŠëŠ” ê³µë°± ë° ì°Œêº¼ê¸° ë¬¸ì ì™„ë²½ ì œê±°
                    df[w_col] = df[w_col].astype(str).str.strip()
                    last_ts_clean = str(last_updated).strip()
                    
                    # â­ï¸ 2. ë‹¨ìˆœ ë¬¸ìì—´ì„ ì‹¤ì œ ì‹œê³„(Datetime ê°ì²´)ë¡œ ê°•ì œ ë³€í™˜
                    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ 1970ë…„ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ì—¬ ë¬´ì¡°ê±´ ìˆ˜ì§‘ë˜ë„ë¡ ë°©ì–´ë§‰ ì „ê°œ
                    df['__parsed_time__'] = pd.to_datetime(df[w_col], errors='coerce').fillna(pd.Timestamp("1970-01-01"))
                    
                    safe_last = pd.to_datetime(last_ts_clean, errors='coerce')
                    if pd.isna(safe_last): 
                        safe_last = pd.Timestamp("1970-01-01")
                        
                    # â­ï¸ 3. ì‹¤ì œ ì‹œê°„ í¬ê¸°ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ë¹„êµí•˜ì—¬ ìµœì‹  ë°ì´í„°ë§Œ ì¶”ì¶œ
                    df = df[df['__parsed_time__'] > safe_last]
                    
                    # ì„ì‹œë¡œ ë§Œë“  ì‹œê°„ ì—°ì‚°ìš© ì»¬ëŸ¼ ì‚­ì œ
                    df = df.drop(columns=['__parsed_time__'])
                    
                    if original_count != len(df):
                        logger.info(f"ğŸ” [{source_name}] ì‹ ê·œ {len(df)}ê±´ ì¶”ì¶œ ì™„ë£Œ (êµ¬ë¶„ì: '{sep}')")

        # HR ì¡°ì¸ 
        if df is not None and not df.empty:
            hr_lookup = get_hr_lookup(global_config)
            if hr_lookup and "user_id" in df.columns:
                df["emp_name"] = df["user_id"].astype(str).map(hr_lookup).fillna("Unknown_User")
        return df
    except Exception as e:
        logger.error(f"âŒ ìˆ˜ì§‘ ì—ëŸ¬: {e}")
        return None