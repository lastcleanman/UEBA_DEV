import pandas as pd
from sqlalchemy import create_engine, text
import json, glob, os
import xml.etree.ElementTree as ET
from core.utils import get_logger

logger = get_logger("Plugin-Input")

def fetch_db(source, last_updated):
    db_type = source.get("type", "").lower()
    db_name = source.get("database")
    table = source.get("table_name")
    
    # â­ï¸ ìˆ˜ì •: DB í…Œì´ë¸”ë„ ê¸°ë³¸ ì›Œí„°ë§ˆí¬ ì»¬ëŸ¼ì„ ì§€ì •í•˜ì—¬ ëˆ„ë½ ë°©ì§€
    w_col = source.get("watermark_col", "timestamp") 
    
    url = f"postgresql+psycopg2://{source['user']}:{source['password']}@{source['host']}:{source['port']}/{db_name}" if "postgres" in db_type else f"mysql+pymysql://{source['user']}:{source['password']}@{source['host']}:{source['port']}/{db_name}"
    
    engine = create_engine(url, pool_pre_ping=True)
    query = f"SELECT * FROM {table} WHERE {w_col} > :last_updated" if w_col else f"SELECT * FROM {table}"
    
    with engine.connect() as conn:
        df = pd.read_sql(text(query), conn, params={"last_updated": last_updated}) if ":last_updated" in query else pd.read_sql(text(query), conn)
    logger.info(f"âœ… [{source['name']}] ì¶”ì¶œ ì™„ë£Œ ({len(df)}ê±´) / í…Œì´ë¸”: {table}")
    return df

def get_hr_lookup(global_config):
    conf = next((s for s in global_config.get("sources", []) if s.get("name") == "ueba_mariaDB"), None)
    if not conf: return None
    try:
        hr_df = fetch_db(conf, "1970-01-01 00:00:00")
        if hr_df is not None and not hr_df.empty:
            id_col = 'employee_id' if 'employee_id' in hr_df.columns else ('emp_id' if 'emp_id' in hr_df.columns else hr_df.columns[0])
            name_col = 'name_kr' if 'name_kr' in hr_df.columns else ('emp_name' if 'emp_name' in hr_df.columns else hr_df.columns[1])
            return dict(zip(hr_df[id_col].astype(str), hr_df[name_col].astype(str)))
    except Exception as e: logger.warning(f"âš ï¸ HR ë¡œë“œ ì‹¤íŒ¨: {e}")
    return None

def fetch_data(source, global_config, last_updated="1970-01-01 00:00:00"):
    source_name = source.get("name")
    try:
        df = None
        if source.get("type") in ["postgres", "mariadb"]: 
            df = fetch_db(source, last_updated)
        elif source.get("type") == "file":
            files = glob.glob(source.get("path", ""))
            
            df_list = []
            for f in files:
                if f.endswith(".json") or f.endswith(".log"):
                    df_list.append(pd.read_json(f, lines=True))
                else:
                    df_list.append(pd.read_csv(f))
                    
            if df_list: 
                df = pd.concat(df_list, ignore_index=True)
                
                # â­ï¸ í•µì‹¬ ìˆ˜ì •: íŒŒì¼ ê¸°ë°˜ ë¡œê·¸ ë°ì´í„°ì—ë„ ì›Œí„°ë§ˆí¬(ì‹œê°„) í•„í„°ë§ ì ìš©!
                w_col = source.get("watermark_col", "timestamp")
                if w_col not in df.columns and "@timestamp" in df.columns:
                    w_col = "@timestamp"
                    
                if w_col in df.columns:
                    original_count = len(df)
                    # ë°ì´í„°í”„ë ˆì„ì—ì„œ 'ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì‹œê°„' ì´í›„ì˜ ë°ì´í„°ë§Œ ì˜ë¼ë‚´ê¸°
                    df = df[df[w_col].astype(str) > str(last_updated)]
                    if original_count != len(df):
                        logger.info(f"ğŸ” [{source_name}] ì›Œí„°ë§ˆí¬ ì ìš©: ì „ì²´ {original_count}ê±´ ì¤‘ ì‹ ê·œ {len(df)}ê±´ë§Œ ì¶”ì¶œ")

        if df is not None and not df.empty:
            hr_lookup = get_hr_lookup(global_config)
            if hr_lookup and "user_id" in df.columns:
                df["emp_name"] = df["user_id"].astype(str).map(hr_lookup).fillna("Unknown_User")
        
        return df
    except Exception as e:
        logger.error(f"âŒ [{source_name}] ìˆ˜ì§‘ ì—ëŸ¬: {e}")
        return None