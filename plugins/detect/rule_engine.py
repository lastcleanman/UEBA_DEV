from pyspark.sql.functions import col, when, lit
from core.utils import get_logger

logger = get_logger("Plugin-RuleEngine")

def execute(spark, df, source_name, config):
    try:
        logger.info("ğŸ•µï¸ [Plugin] ë£° ê¸°ë°˜ ìœ„í˜‘ ë¶„ì„(Rule Engine) ì‹œì‘...")
        
        # ê¸°ë³¸ ì»¬ëŸ¼ì´ ì—†ë‹¤ë©´ ìƒì„±
        if "risk_score" not in df.columns:
            df = df.withColumn("risk_score", lit(0.0))
        if "alert_reason" not in df.columns:
            df = df.withColumn("alert_reason", lit(""))

        # ---------------------------------------------------------
        # [íƒì§€ ë£°ì…‹ 1] ë¡œê·¸ì¸ ì‹¤íŒ¨(LOGIN_FAILED) íƒì§€
        # ---------------------------------------------------------
        if "action" in df.columns:
            df = df.withColumn(
                "risk_score",
                when(col("action").rlike("(?i)fail|error|deny|block"), col("risk_score") + 30.0)
                .otherwise(col("risk_score"))
            )
            df = df.withColumn(
                "alert_reason",
                when(col("action").rlike("(?i)fail|error|deny|block"), 
                     when(col("alert_reason") == "", "Login/Access Failed")
                     .otherwise(col("alert_reason"))
                ).otherwise(col("alert_reason"))
            )
            
        logger.info("âœ… [Plugin] ìœ„í˜‘ ë¶„ì„ ë° ìŠ¤ì½”ì–´ë§ ì™„ë£Œ")
        return df

    except Exception as e:
        logger.error(f"âŒ [Plugin] Rule Engine ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {e}")
        return df # ì—ëŸ¬ê°€ ë‚˜ë„ íŒŒì´í”„ë¼ì¸ì´ ëŠê¸°ì§€ ì•Šë„ë¡ ì›ë³¸ ë°˜í™˜