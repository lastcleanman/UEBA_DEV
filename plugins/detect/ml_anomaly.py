from pyspark.sql.functions import col, hour, when, count, lit, to_timestamp
from pyspark.sql.window import Window
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml.clustering import KMeans
from core.utils import get_logger

logger = get_logger("Plugin-MLAnomaly")

def execute(spark, df, source_name, config):
    try:
        logger.info("ğŸ¤– [Plugin] ë¨¸ì‹ ëŸ¬ë‹(ML) ê¸°ë°˜ ì´ìƒ í–‰ìœ„ ë¶„ì„ ì‹œì‘...")
        
        total_count = df.count()
        if total_count < 2:  # K-Means(K=2)ëŠ” ìµœì†Œ 2ê±´ ì´ìƒì˜ ë°ì´í„°ê°€ í•„ìš”í•¨
            logger.info("â© ë°ì´í„° ê±´ìˆ˜ê°€ ë¶€ì¡±í•˜ì—¬ ML ë¶„ì„ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            return df

        # 1. ì‹œê°„ ì»¬ëŸ¼ í™•ë³´
        ts_cols = [c for c in df.columns if "timestamp" in c.lower() or "ts" in c.lower()]
        if not ts_cols:
            return df
            
        # 2. íŠ¹ì§•(Feature) ì¶”ì¶œ ë° ê²°ì¸¡ì¹˜(Null) ì™„ë²½ ë°©ì–´ â­ï¸
        df = df.withColumn("_ml_ts", to_timestamp(col(ts_cols[0])))
        df = df.withColumn("_hour", hour(col("_ml_ts")))
        df = df.fillna(0, subset=["_hour"])  # ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨ ì‹œ 0ì‹œ(ìì •)ë¡œ ê¸°ë³¸ê°’ ì²˜ë¦¬
        
        # ë¬¸ìë¡œ ëœ actionì„ ìˆ«ìë¡œ ë³€í™˜
        if "action" in df.columns:
            indexer = StringIndexer(inputCol="action", outputCol="_action_idx", handleInvalid="keep")
            df = indexer.fit(df).transform(df)
        else:
            df = df.withColumn("_action_idx", lit(0.0))

        # ì‹œê°„ëŒ€ì™€ í–‰ë™ì„ ë²¡í„°ë¡œ ê²°í•© (handleInvalid="keep"ìœ¼ë¡œ ì—ëŸ¬ ë°©ì§€ â­ï¸)
        assembler = VectorAssembler(inputCols=["_hour", "_action_idx"], outputCol="_features", handleInvalid="keep")
        ml_df = assembler.transform(df)

        # 3. ë¹„ì§€ë„ í•™ìŠµ (K-Means Clustering)
        kmeans = KMeans(k=2, seed=42, featuresCol="_features", predictionCol="_cluster")
        model = kmeans.fit(ml_df)
        pred_df = model.transform(ml_df)

        # 4. ì´ìƒ íƒì§€ (ì „ì²´ ë°ì´í„°ì˜ 20% ë¯¸ë§Œì¸ ì†Œìˆ˜ êµ°ì§‘ì„ ì´ìƒí–‰ìœ„ë¡œ ê°„ì£¼)
        win = Window.partitionBy("_cluster")
        pred_df = pred_df.withColumn("_cluster_size", count("*").over(win))
        
        threshold = max(total_count * 0.2, 1)

        pred_df = pred_df.withColumn(
            "risk_score_ml",
            when(col("_cluster_size") < threshold, 40.0).otherwise(0.0)
        )

        # 5. ìµœì¢… ìœ„í˜‘ ì ìˆ˜ í•©ì‚°
        pred_df = pred_df.withColumn("risk_score", col("risk_score") + col("risk_score_ml"))
        pred_df = pred_df.withColumn(
            "alert_reason",
            when(col("risk_score_ml") > 0, 
                 when(col("alert_reason") == "", "ML Anomaly (Rare Behavior)")
                 .otherwise(col("alert_reason"))
            ).otherwise(col("alert_reason"))
        )

        # 6. ì„ì‹œ ì»¬ëŸ¼ ê¹”ë”í•˜ê²Œ ì •ë¦¬
        final_df = pred_df.drop("_ml_ts", "_hour", "_action_idx", "_features", "_cluster", "_cluster_size", "risk_score_ml")
        
        logger.info("âœ… [Plugin] ë¨¸ì‹ ëŸ¬ë‹ ì´ìƒ í–‰ìœ„ ë¶„ì„ ì™„ë£Œ ë° ìœ„í˜‘ ì ìˆ˜ ë¶€ì—¬ ì„±ê³µ")
        return final_df

    except Exception as e:
        logger.error(f"âŒ [Plugin] ML ë¶„ì„ ì¤‘ ì—ëŸ¬: {e}")
        return df