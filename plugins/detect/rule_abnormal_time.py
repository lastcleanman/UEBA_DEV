from pyspark.sql.functions import col, hour, dayofweek, when, to_timestamp
from core.utils import get_logger

logger = get_logger("Plugin-AbnormalTime")

def execute(df, config=None):
    try:
        logger.info("üïí [Plugin] Ïã¨Ïïº/Ï£ºÎßê ÎπÑÏ†ïÏÉÅ Ï†ëÍ∑º Î∂ÑÏÑù Ï§ë...")
        
        # ‚≠êÔ∏è ÎåÄÏÜåÎ¨∏Ïûê Íµ¨Î∂Ñ ÏóÜÏù¥ timestampÎÇò tsÍ∞Ä Ìè¨Ìï®Îêú Î™®Îì† Ïª¨Îüº Í≤ÄÏÉâ
        potential_cols = [c for c in df.columns if "timestamp" in c.lower() or "ts" in c.lower()]
        target_ts_col = potential_cols[0] if potential_cols else None
        
        if not target_ts_col:
            logger.warning(f"‚ö†Ô∏è ÏãúÍ∞Ñ Ïª¨ÎüºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. (ÌòÑÏû¨ ÌïÑÎìú: {df.columns})")
            return df

        # ÏãúÍ∞Ñ ÌÉÄÏûÖÏúºÎ°ú Î≥ÄÌôò Î∞è Î∂ÑÏÑù
        df = df.withColumn("_temp_ts", to_timestamp(col(target_ts_col)))
        df = df.withColumn("hour", hour(col("_temp_ts")))
        df = df.withColumn("day_of_week", dayofweek(col("_temp_ts")))
        
        # Ï£ºÎßê(1,7) ÎòêÎäî Ïã¨Ïïº(0~6Ïãú) Ïä§ÏΩîÏñ¥ÎßÅ
        df = df.withColumn(
            "risk_score_time",
            when((col("day_of_week").isin([1, 7])), 30.0)
            .when((col("hour") >= 0) & (col("hour") <= 6), 50.0)
            .otherwise(0.0)
        )
        
        df = df.withColumn("risk_score", col("risk_score") + col("risk_score_time"))
        df = df.withColumn(
            "alert_reason",
            when(col("risk_score_time") > 0, "Abnormal Time Access").otherwise(col("alert_reason"))
        )
        
        return df.drop("_temp_ts", "hour", "day_of_week", "risk_score_time")
    except Exception as e:
        logger.error(f"‚ùå Î∂ÑÏÑù Ïã§Ìå®: {e}")
        return df