import pandas as pd
from datetime import datetime
from sqlalchemy import create_engine, text
from backend.core.utils import get_logger

logger = get_logger("Plugin-MLAnomaly")

def get_db_engine(config):
    conf = next((s for s in config.get("sources", []) if s.get("name") == "ueba_mariaDB"), None)
    if not conf: return None
    url = f"mysql+pymysql://{conf['user']}:{conf['password']}@{conf['host']}:{conf['port']}/{conf['database']}"
    return create_engine(url, pool_pre_ping=True)

def execute(spark, current_df, source_name, config):
    try:
        # 1. Spark DF -> íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ -> Pandas DF (ì•ˆì „í•œ ë©”ëª¨ë¦¬ ë³€í™˜)
        rows = current_df.collect()
        if not rows: return current_df
        
        data_list = [row.asDict() for row in rows]
        df = pd.DataFrame(data_list)

        current_hour = datetime.now().hour
        is_night = 1 if 0 <= current_hour <= 5 else 0
        
        # 'user' ì»¬ëŸ¼ì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë°©ì–´ ì½”ë“œ
        user_counts = df.groupby('user').size() if 'user' in df.columns else pd.Series(dtype=int)
        avg_batch_count = user_counts.mean() if not user_counts.empty else 0
        
        logger.info(f"ğŸ¤– [{source_name}] ë¨¸ì‹ ëŸ¬ë‹(ML) ê¸°ë°˜ ì§€í‘œ ë¶„ì„ ì¤‘... (ëŒ€ìƒ: {len(df)}ê±´)")

        # â­ï¸ AI ìì—°ì–´ ì‚¬ìœ  ìƒì„± ë¡œì§
        def calculate_score(row):
            # ì´ì „ ë‹¨ê³„(rule_engine ë“±)ì—ì„œ ëˆ„ì ëœ ìŠ¤ì½”ì–´ í™•ë³´
            score = float(row.get('risk_score', 0.0))
            contexts = []
            
            user = row.get('user', '')
            if user and not user_counts.empty:
                user_count = user_counts.get(user, 0)
                if avg_batch_count > 0 and user_count > (avg_batch_count * 3):
                    score += 40
                    contexts.append(f"í‰ì†Œ ëŒ€ë¹„ ë¹„ì •ìƒì ì¸ í–‰ìœ„ í­ì¦({user_count}ê±´)")
            
            res_val = str(row.get('resource', '')).lower()
            if any(ext in res_val for ext in ['.sql', 'admin', 'backup']):
                score += 50
                contexts.append(f"ì¸ê°€ë˜ì§€ ì•Šì€ ë¯¼ê° ë¦¬ì†ŒìŠ¤({res_val}) ì ‘ê·¼")
                
            if is_night:
                score += 20
                contexts.append("ë¹„ì—…ë¬´ ì‹¬ì•¼ ì‹œê°„ëŒ€(00~05ì‹œ) í™œë™")
                
            if score >= 70:
                ai_reason = f"[AI í–‰ìœ„ ë¶„ì„] {', '.join(contexts)} íŒ¨í„´ì´ ë³µí•©ì ìœ¼ë¡œ ì‹ë³„ë˜ì—ˆìŠµë‹ˆë‹¤. ë‚´ë¶€ì ê¶Œí•œ ë‚¨ìš© ë˜ëŠ” ìê²© ì¦ëª… íƒˆì·¨ê°€ ì˜ì‹¬ë©ë‹ˆë‹¤."
            else:
                ai_reason = ", ".join(contexts) if contexts else "ì •ìƒ ë²”ì£¼"
                
            return pd.Series([score, ai_reason])

        # Pandas apply ê²°ê³¼ë¥¼ ë‘ ê°œì˜ ì»¬ëŸ¼ìœ¼ë¡œ ê¹”ë”í•˜ê²Œ í™•ì¥(expand)
        df[['risk_score', 'anomaly_reason']] = df.apply(calculate_score, axis=1, result_type='expand')
        
        # 2. ê³ ìœ„í—˜êµ° DB ì ì¬ (HR ë°ì´í„° ì¡°ì¸)
        db_engine = get_db_engine(config)
        anomalies = df[df['risk_score'] >= 70].copy()
        
        if not anomalies.empty and db_engine:
            with db_engine.begin() as conn:
                for _, row in anomalies.iterrows():
                    conn.execute(text("""
                        INSERT INTO sj_ueba_anomalies (
                            user, risk_score, anomaly_reason, source_name, timestamp,
                            emp_id, dept_name, dept_code
                        )
                        SELECT 
                            :u, :s, :r, :src, NOW(),
                            h.emp_id, h.dept_name, h.dept_code
                        FROM (SELECT :u AS u_name) AS tmp
                        LEFT JOIN sj_ueba_hr h ON h.user_name = tmp.u_name
                    """), {
                        "u": row.get('user', 'Unknown'), 
                        "s": row['risk_score'], 
                        "r": row['anomaly_reason'], 
                        "src": source_name
                    })
            logger.warning(f"ğŸš¨ [Anomaly Detected] {len(anomalies)}ê±´ ì ë°œ ë° DB ê¸°ë¡ ì™„ë£Œ ({source_name})")

        # 3. Spark í˜¸í™˜ì„±ì„ ìœ„í•œ ê²°ì¸¡ì¹˜ ì •ë¦¬ í›„ ë³µêµ¬
        df = df.fillna("").astype(str)
        df = df.replace({'nan': '', 'None': '', '<NA>': ''})
        
        dict_list = df.to_dict(orient='records')
        return spark.createDataFrame(dict_list) if dict_list else current_df

    except Exception as e:
        logger.error(f"âŒ ML Anomaly ë¶„ì„ ëª¨ë“ˆ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return current_df