from pyspark.sql.functions import col, when, lit
from backend.core.utils import get_logger

logger = get_logger("Plugin-Detect")

def execute(spark, df, global_config):
    logger.info(f"ğŸš¨ [Step 3] ë£° ê¸°ë°˜ ìœ„í˜‘ íƒì§€(Rule Engine) ì‹œì‘ (ë¶„ì„ ëŒ€ìƒ: {df.count()}ê±´)")
    
    try:
        # 1. ìœ„í˜‘ íƒì§€ ë£° ì ìš©: ì›¹ íƒì§€(DETECT) ì´ë²¤íŠ¸ê°€ ë°œìƒí•œ ê²½ìš° ìœ„í—˜ë„ 80ì  ë¶€ì—¬
        df_scored = df.withColumn(
            "risk_score",
            when(col("event_type") == "DETECT", lit(80)).otherwise(lit(0))
        ).withColumn(
            "threat_name",
            when(col("event_type") == "DETECT", lit("ì›¹ ê³µê²© (WAF ì°¨ë‹¨/íƒì§€)")).otherwise(lit("Normal"))
        )
        
        # 2. ìœ„í—˜ ì ìˆ˜ê°€ 0ì  ì´ˆê³¼ì¸ 'ì§„ì§œ ìœ„í˜‘'ë§Œ ê±¸ëŸ¬ë‚´ê¸°
        anomaly_df = df_scored.filter(col("risk_score") > 0)
        anomaly_count = anomaly_df.count()
        
        logger.info(f"âœ… [Step 3] ìœ„í˜‘ íƒì§€ ì™„ë£Œ! (ë°œê²¬ëœ ì´ìƒ í–‰ìœ„: {anomaly_count}ê±´)")
        
        if anomaly_count > 0:
            logger.info("ğŸ”¥ [íƒì§€ëœ ìœ„í˜‘ ìƒ˜í”Œ]")
            anomaly_df.show(5, truncate=False)
            
        return anomaly_df

    except Exception as e:
        logger.error(f"âŒ [Step 3] íƒì§€ ì—”ì§„ ì˜¤ë¥˜: {e}")
        return df