import os
import json
import shutil
import importlib
from datetime import datetime
import pandas as pd
from sqlalchemy import create_engine, text
from backend.core.utils import get_logger, get_spark_session
from backend.core.config import INTERMEDIATE_PATH, WATERMARK_FILE

logger = get_logger("PipelineExecutor")

class PipelineExecutor:
    def __init__(self, config, plugin_manager):
        self.config = config
        self.pm = plugin_manager
        self.db_engine = self._get_db_engine()
        os.makedirs(INTERMEDIATE_PATH, exist_ok=True)

    def _get_db_engine(self):
        conf = next((s for s in self.config.get("sources", []) if s.get("name") == "ueba_mariaDB"), None)
        if not conf: return None
        url = f"mysql+pymysql://{conf['user']}:{conf['password']}@{conf['host']}:{conf['port']}/{conf['database']}"
        return create_engine(url, pool_pre_ping=True)

    def _save_history(self, source, count, status, error="", start_time=None):
        if self.db_engine is None: return
        try:
            with self.db_engine.begin() as conn:
                conn.execute(text("""
                    INSERT INTO sj_ueba_ingestion_history (source_name, processed_count, status, error_message, start_time)
                    VALUES (:s, :c, :st, :e, :t)
                """), {"s": source, "c": count, "st": status, "e": error, "t": start_time})
            logger.info(f"ğŸ“œ [History] {source} ì™„ë£Œ ({count}ê±´)")
        except Exception as e: logger.warning(f"âš ï¸ ì´ë ¥ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _get_last_ts(self, source_name):
        try:
            if os.path.exists(WATERMARK_FILE):
                with open(WATERMARK_FILE, "r") as f: return json.load(f).get(source_name, "1970-01-01 00:00:00")
        except: pass
        return "1970-01-01 00:00:00"

    def _set_last_ts(self, source_name, ts):
        try:
            os.makedirs(os.path.dirname(WATERMARK_FILE), exist_ok=True)
            data = {}
            if os.path.exists(WATERMARK_FILE):
                with open(WATERMARK_FILE, "r") as f:
                    try: data = json.load(f)
                    except: data = {}
            data[source_name] = str(ts)
            with open(WATERMARK_FILE, "w") as f: json.dump(data, f, indent=4)
        except Exception as e: logger.warning(f"âš ï¸ ì›Œí„°ë§ˆí¬ ì €ì¥ ì‹¤íŒ¨: {e}")

    def run_input(self):
        sources = [s for s in self.config.get("sources", []) if s.get("enabled", True)]
        input_plugins = self.pm.load_plugins("input")
        if not input_plugins: return
        input_plugin = importlib.import_module(input_plugins[0])

        for source in sources:
            start_time = datetime.now()
            source_name = source.get('name')
            watermark_col = source.get("watermark_col", "timestamp")
            out_path = os.path.join(INTERMEDIATE_PATH, f"{source_name}_input.parquet")

            try:
                last_ts = self._get_last_ts(source_name)
                if last_ts == "1970-01-01 00:00:00":
                    last_ts = source.get("watermark_default", "1970-01-01 00:00:00")

                raw_pandas_df = input_plugin.fetch_data(source, self.config, last_updated=last_ts)

                if raw_pandas_df is None or raw_pandas_df.dropna(axis=1, how='all').empty:
                    logger.info(f"â© [{source_name}] ì‹ ê·œ ìˆ˜ì§‘ ë°ì´í„° ì—†ìŒ")
                    self._save_history(source_name, 0, "SUCCESS", start_time=start_time)
                    for suffix in ["_input.parquet", "_process.parquet", "_detect.parquet"]:
                        ghost_file = os.path.join(INTERMEDIATE_PATH, f"{source_name}{suffix}")
                        if os.path.exists(ghost_file):
                            if os.path.isdir(ghost_file): shutil.rmtree(ghost_file)
                            else: os.remove(ghost_file)
                    continue

                if watermark_col not in raw_pandas_df.columns and '@timestamp' in raw_pandas_df.columns:
                    watermark_col = '@timestamp'

                if watermark_col in raw_pandas_df.columns:
                    new_ts = str(raw_pandas_df[watermark_col].max())
                    self._set_last_ts(source_name, new_ts)
                    logger.info(f"ğŸ•’ [{source_name}] ì›Œí„°ë§ˆí¬ ê°±ì‹  ì™„ë£Œ: {new_ts}")

                raw_pandas_df.to_parquet(out_path, index=False)
                logger.info(f"âœ… [{source_name}] ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ ({len(raw_pandas_df)}ê±´)")

            except Exception as e:
                logger.error(f"âŒ [{source_name}] Input ì—ëŸ¬: {e}")
                self._save_history(source_name, 0, "FAIL", str(e), start_time=start_time)

    def run_process(self, spark):
        sources = [s for s in self.config.get("sources", []) if s.get("enabled", True)]
        for source in sources:
            source_name = source.get('name')
            in_path = os.path.join(INTERMEDIATE_PATH, f"{source_name}_input.parquet")
            out_path = os.path.join(INTERMEDIATE_PATH, f"{source_name}_process.parquet")

            if not os.path.exists(in_path): continue

            try:
                raw_pandas_df = pd.read_parquet(in_path)
                raw_pandas_df = raw_pandas_df.fillna("").astype(str)
                raw_pandas_df = raw_pandas_df.replace({'nan': '', 'None': '', '<NA>': ''})

                dict_list = raw_pandas_df.to_dict(orient='records')
                if not dict_list: continue

                spark_df = spark.createDataFrame(dict_list)
                clean_df = self.pm.execute_plugins(spark, spark_df, "process", source_name)

                if clean_df.count() > 0:
                    clean_df.write.mode("overwrite").parquet(out_path)
                    logger.info(f"âœ… [{source_name}] ë°ì´í„° ì •ì œ ì™„ë£Œ")
            except Exception as e: logger.error(f"âŒ [{source_name}] Process ì—ëŸ¬: {e}")

    def run_detect(self, spark):
        sources = [s for s in self.config.get("sources", []) if s.get("enabled", True)]
        for source in sources:
            source_name = source.get('name')
            in_path = os.path.join(INTERMEDIATE_PATH, f"{source_name}_process.parquet")
            out_path = os.path.join(INTERMEDIATE_PATH, f"{source_name}_detect.parquet")

            if not os.path.exists(in_path): continue

            try:
                clean_df = spark.read.parquet(in_path)
                detected_df = self.pm.execute_plugins(spark, clean_df, "detection", source_name)
                detected_df.write.mode("overwrite").parquet(out_path)
                logger.info(f"âœ… [{source_name}] AI ìœ„í˜‘ ë¶„ì„ ì™„ë£Œ")
            except Exception as e: logger.error(f"âŒ [{source_name}] Detect ì—ëŸ¬: {e}")

    def run_output(self, spark):
        sources = [s for s in self.config.get("sources", []) if s.get("enabled", True)]
        for source in sources:
            start_time = datetime.now()
            source_name = source.get('name')
            in_path = os.path.join(INTERMEDIATE_PATH, f"{source_name}_detect.parquet")

            if not os.path.exists(in_path): continue

            try:
                detected_df = spark.read.parquet(in_path)
                self.pm.execute_plugins(spark, detected_df, "output", source_name)
                
                count = detected_df.count()
                self._save_history(source_name, count, "SUCCESS", start_time=start_time)
            except Exception as e:
                self._save_history(source_name, 0, "FAIL", str(e), start_time=start_time)
    
    def execute(self):
        """íŒŒì´í”„ë¼ì¸ ì „ì²´ ë‹¨ê³„ë¥¼ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        logger.info("ğŸš€ UEBA íŒŒì´í”„ë¼ì¸ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # 1. Spark ì„¸ì…˜ ìƒì„±
        spark = get_spark_session()
        
        try:
            # 2. ë‹¨ê³„ë³„ ë©”ì„œë“œ í˜¸ì¶œ
            logger.info("Step 1: ë°ì´í„° ìˆ˜ì§‘(Input) ì‹œì‘")
            self.run_input()
            
            logger.info("Step 2: ë°ì´í„° ì •ì œ(Process) ì‹œì‘")
            self.run_process(spark)
            
            logger.info("Step 3: ìœ„í˜‘ íƒì§€(Detect) ì‹œì‘")
            self.run_detect(spark)
            
            logger.info("Step 4: ìµœì¢… ì ì¬(Output) ì‹œì‘")
            self.run_output(spark)
            
            logger.info("âœ… íŒŒì´í”„ë¼ì¸ì˜ ëª¨ë“  ë‹¨ê³„ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"âš ï¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì—ëŸ¬ ë°œìƒ: {e}")
        finally:
            # ìì› ê´€ë¦¬ë¥¼ ìœ„í•´ í•„ìš”í•œ ê²½ìš° ì„¸ì…˜ ì²˜ë¦¬ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
            pass