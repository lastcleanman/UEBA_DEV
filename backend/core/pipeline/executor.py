import sys
sys.path.insert(0, "/UEBA_DEV")

import json
import importlib
import functools
from pyspark.sql import SparkSession, DataFrame
from backend.core.utils import get_logger
from backend.core.config import CONFIG_FILE

logger = get_logger("PipelineExecutor")

class UEBAPipeline:
    def __init__(self):
        self.spark = SparkSession.builder.appName("UEBA_Core_Engine").getOrCreate()
        try:
            with open(CONFIG_FILE, "r", encoding="utf-8") as f:
                self.config = json.load(f)
        except Exception as e:
            logger.error(f"âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.config = {}

    def run_pipeline(self):
        logger.info("ğŸš€ UEBA ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê°€ë™ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        try:
            # ---------------------------------------------------------
            # 1ë‹¨ê³„: ìˆ˜ì§‘ (Input)
            # ---------------------------------------------------------
            logger.info("â–¶ï¸ [Step 1] ë°ì´í„° ìˆ˜ì§‘ ë° DB ì´ë ¥ ì ì¬ ì‹œì‘")
            input_plugins = self.config.get("pipeline", {}).get("input", [])
            
            collected_data = [] # â­ï¸ (ì†ŒìŠ¤ì´ë¦„, ë°ì´í„°í”„ë ˆì„) ìŒìœ¼ë¡œ ì €ì¥
            for plugin_path in input_plugins:
                plugin_module = importlib.import_module(plugin_path)
                for source in self.config.get("sources", []):
                    if source.get("type") == "file":
                        df = plugin_module.execute(self.spark, source, self.config)
                        if df and not df.isEmpty():
                            collected_data.append({"name": source.get("name"), "df": df})
            
            if not collected_data:
                logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ì–´ íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return

            logger.info(f"âœ… 1ë‹¨ê³„ ì™„ë£Œ: ì´ {len(collected_data)}ê°œ ì†ŒìŠ¤ ìˆ˜ì§‘ ì„±ê³µ!")

            # ---------------------------------------------------------
            # 2ë‹¨ê³„: Parser (Process) - ê°œë³„ ì •ì œ í›„ ë³‘í•©
            # ---------------------------------------------------------
            logger.info("â–¶ï¸ [Step 2] Parser ê¸°ë°˜ ë°ì´í„° ì •ì œ ì‹œì‘")
            process_module = importlib.import_module("backend.plugins.process.normalizer")
            
            parsed_dfs = []
            for item in collected_data:
                # â­ï¸ ëª¨ì–‘ì´ ë‹¤ë¥¸ ê° ë¡œê·¸ë¥¼ ë¨¼ì € í‘œì¤€ í¬ë§·ìœ¼ë¡œ ê¹ì•„ëƒ…ë‹ˆë‹¤.
                parsed_df = process_module.execute(self.spark, item["df"], item["name"])
                if parsed_df and not parsed_df.isEmpty():
                    parsed_dfs.append(parsed_df)

            if not parsed_dfs:
                logger.warning("âš ï¸ ì •ì œëœ ìœ íš¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            # â­ï¸ ì´ì œ ëª¨ë“  ë¡œê·¸ê°€ 5ì¹¸ì§œë¦¬ í‘œì¤€ í¼ìœ¼ë¡œ ë˜‘ê°™ì•„ì¡Œìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤!
            main_df = functools.reduce(DataFrame.unionByName, parsed_dfs)
            
            logger.info("âœ… 2ë‹¨ê³„ ì •ì œ ë° ë³‘í•© ì™„ë£Œ! (ì•„ë˜ëŠ” ì •ì œëœ ìƒ˜í”Œ ë°ì´í„°ì…ë‹ˆë‹¤)")
            #main_df.show(10, truncate=False)

            # ---------------------------------------------------------
            # 3ë‹¨ê³„: ë¶„ì„ (Rule-based Detect)
            # ---------------------------------------------------------
            logger.info("â–¶ï¸ [Step 3] ë£° ê¸°ë°˜ ìœ„í˜‘ ë¶„ì„ ì‹œì‘")
            rule_module = importlib.import_module("backend.plugins.detect.rule_engine")
            
            # íƒì§€ ëª¨ë“ˆ ì‹¤í–‰ (ìœ„í—˜ ë°ì´í„°ë§Œ ë¦¬í„´ë¨)
            anomaly_df = rule_module.execute(self.spark, main_df, self.config)
            
            if anomaly_df and not anomaly_df.isEmpty():
                logger.info(f"ğŸ¯ íŒŒì´í”„ë¼ì¸ ë¶„ì„ ê²°ê³¼: ì´ {anomaly_df.count()}ê±´ì˜ ìœ„í˜‘ì´ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                logger.info("ğŸ•Šï¸ íƒì§€ëœ ìœ„í˜‘ì´ ì—†ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œì´ ì•ˆì „í•©ë‹ˆë‹¤.")

            # [í–¥í›„ 4ë‹¨ê³„: ì ì¬(Output) í”ŒëŸ¬ê·¸ì¸ì´ ì—¬ê¸°ì— ì—°ê²°ë©ë‹ˆë‹¤]

            
        except Exception as e:
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            self.spark.stop()

if __name__ == "__main__":
    pipeline = UEBAPipeline()
    pipeline.run_pipeline()