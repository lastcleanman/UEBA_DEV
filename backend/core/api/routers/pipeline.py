import subprocess
import datetime
from fastapi import APIRouter, BackgroundTasks, HTTPException
from backend.core.config import SPARK_JOB_SCRIPT
from backend.core.utils import get_logger

router = APIRouter(prefix="/api/v1/pipeline", tags=["Pipeline Execution"])
logger = get_logger("API_Pipeline")

# â­ï¸ íŒŒì´í”„ë¼ì¸ ìƒíƒœë¥¼ ì €ì¥í•˜ëŠ” ë©”ëª¨ë¦¬ ì €ì¥ì†Œ (DB ì—°ë™ ì „ ê°€ë²¼ìš´ í˜•íƒœ)
pipeline_state = {
    "is_running": False,
    "current_stage": "idle", # idle, input, process, detect, output, done
    "start_time": None,
    "end_time": None,
    "last_log": "ëŒ€ê¸° ì¤‘",
    "last_exit_code": None
}

def run_spark_job_in_background():
    global pipeline_state
    pipeline_state["is_running"] = True
    pipeline_state["current_stage"] = "init"
    pipeline_state["start_time"] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    pipeline_state["message"] = "Spark ë¶„ì„ íŒŒì´í”„ë¼ì¸ì´ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤..."
    pipeline_state["end_time"] = None
    
    submit_cmd = [
        "spark-submit",
        "--master", "spark://ueba-spark:7077",
        "--executor-memory", "1g",
        SPARK_JOB_SCRIPT
    ]
    logger.info(f"â–¶ï¸ [API Trigger] ë°±ê·¸ë¼ìš´ë“œ Spark-Submit ì‹¤í–‰: {' '.join(submit_cmd)}")
    
    try:
        process = subprocess.Popen(
            submit_cmd, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.STDOUT,
            universal_newlines=True
        )
        
        for line in process.stdout:
            clean_line = line.strip()
            print(f"[Spark-Job] {clean_line}")
            
            pipeline_state["last_log"] = clean_line # í”„ë¡ íŠ¸ì—”ë“œ í„°ë¯¸ë„ìš© ì‹¤ì‹œê°„ ë¡œê·¸
            
            # ë‹¨ê³„ ì¶”ì  ë¡œì§
            if "Step 1: ë°ì´í„° ìˆ˜ì§‘" in clean_line:
                pipeline_state["current_stage"] = "input"
            elif "Step 2: ë°ì´í„° ì •ì œ" in clean_line:
                pipeline_state["current_stage"] = "process"
            elif "Step 3: ìœ„í˜‘ íƒì§€" in clean_line:
                pipeline_state["current_stage"] = "detect"
            elif "Step 4: ìµœì¢… ì ì¬" in clean_line:
                pipeline_state["current_stage"] = "output"
            elif "ëª¨ë“  ë‹¨ê³„ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ" in clean_line:
                pipeline_state["current_stage"] = "done"
            
        process.wait()
        
        # â­ï¸ ì‘ì—… ì¢…ë£Œ í›„ ìƒíƒœ ì—…ë°ì´íŠ¸
        pipeline_state["is_running"] = False
        pipeline_state["end_time"] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        pipeline_state["last_exit_code"] = process.returncode
        
        if process.returncode == 0:
            pipeline_state["message"] = "âœ… íŒŒì´í”„ë¼ì¸ ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
            logger.info("âœ… ë°±ê·¸ë¼ìš´ë“œ íŒŒì´í”„ë¼ì¸ ë¶„ì„ ì¢…ë£Œ.")
        else:
            pipeline_state["message"] = f"âŒ Spark Job ë¹„ì •ìƒ ì¢…ë£Œ (Exit Code: {process.returncode})"
            logger.error(pipeline_state["message"])
            
    except Exception as e:
        pipeline_state["is_running"] = False
        pipeline_state["message"] = f"âŒ ì¹˜ëª…ì  ì‹¤í–‰ ì˜¤ë¥˜: {e}"
        logger.error(pipeline_state["message"])

@router.post("/run")
async def trigger_pipeline(background_tasks: BackgroundTasks):
    """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì„ ìš”ì²­í•©ë‹ˆë‹¤."""
    global pipeline_state
    
    if pipeline_state["is_running"]:
        raise HTTPException(status_code=400, detail="ì´ë¯¸ íŒŒì´í”„ë¼ì¸ì´ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ëë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
        
    logger.info("ğŸŒ íŒŒì´í”„ë¼ì¸ êµ¬ë™ API í˜¸ì¶œ ìˆ˜ì‹  (POST /api/v1/pipeline/run)")
    background_tasks.add_task(run_spark_job_in_background)
    
    return {
        "status": "success", 
        "message": "íŒŒì´í”„ë¼ì¸ ë¶„ì„ ì‘ì—…ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
    }

# â­ï¸ ìƒˆë¡œ ì¶”ê°€ëœ ìƒíƒœ ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸
@router.get("/status")
async def get_pipeline_status():
    """í˜„ì¬ íŒŒì´í”„ë¼ì¸ì˜ ì‹¤í–‰ ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return pipeline_state